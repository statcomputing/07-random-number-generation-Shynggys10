---
title: "Homework 7"
author: "Shynggys Magzanov"
date: "23 10 2020"
output: html_document
---


## Exercise 5.3.1. \
### Step 1. Given two pdfs f and g, we need to find the normalizing constant for g. Additionally, we need to show that g is actually a mixture of gamma distributions.

It can be clearly seen that we can seperate the formula $g(x)\propto(2x^{\theta-1}+x^{\theta-1/2})e^{-x}$ into two Gamma distributions. 
Then we can get $2C\Gamma(\theta)+C\Gamma(\theta+\frac{1}{2})=1$.
Therefore, $C=\frac{1}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}$
$$g(x)=\frac{1}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}2x^{\theta-1}e^{-x}+\frac{1}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}x^{\theta-1/2}e^{-x}$$
$$=\frac{2\Gamma(\theta)}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}\cdot\frac{1}{\Gamma(\theta)}{x^{\theta-1}e^{-x}}+\frac{\Gamma(\theta+\frac{1}{2})}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}\cdot\frac{1}{\Gamma(\theta+\frac{1}{2})}{2x^{\theta-1/2}e^{-x}}$$
We can conclude that $g$ is a mixture of Gamma($\theta$,1) and Gamma($\theta+\frac{1}{2}$,1) with the weights $\frac{2\Gamma(\theta)}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}$ and $\frac{\Gamma(\theta+\frac{1}{2})}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}$ respectively. 

### Step 2. We need to write a code to generate pseudo-random sample of size 10000 from distribution with density g. Additionally, we need to plot kernel density and real density on the same graph.

In this particular implementation I used $\theta=5$ and added a histogram of generated sample.

```{r}

g <- function(x, theta = 5){
  (1/(2*gamma(theta)+gamma(theta+0.5)))*(2*x^(theta-1)+x^(theta-0.5))*exp(-x)
}

sample_g <- function(theta){
  w <- 2*gamma(theta)/(2*gamma(theta)+gamma(theta+1/2))
  m <- 10000
  counts <- 0
  draws <- c()
  u <- runif(m,0,1)
  for(i in 1:m){
    if(u[i]<w){
      x <- rgamma(1,theta,1)
      counts <- counts+1
      draws <- c(draws,x)
    }
    else{
      x <- rgamma(1,theta+0.5,1)
    counts <- counts+1
    draws <- c(draws,x)  
    }
  }
  return(draws)
}
X <- sample_g(5)
hist(X,prob=TRUE,main = "Estimated Kernel density with theta=5")
lines(density(X))

curve(g,from = 0,to=20,add=T,col="red")
legend("topright", inset=.1, 
         legend = c("kernel density","true density"), 
         bty = "n", lty = 1, lwd = 2, col = c("black", "red"))
```

We can see empirically that that the generated sample indeed seems to be from density $g$.

### Step 3. We need to use rejection sampling to sample from f using g as the instrumental distribution. And we need to plot estimated kernel kernel density of a randomly generated sample with real f on the same graph.


Let us denote $f(x) = \frac{q(x)}{C_1}$, 
and $q(x) = \sqrt{x + 4} x^{\theta-1} e^{-x}$\
Firstly,  we need to find the value of $\alpha$
$$\alpha = sup \frac {q(x)} {g(x)}
         = sup \frac {\sqrt{x + 4} x^{\theta-1} e^{-x}} 
         {C(2x^{\theta - 1} + x^{\theta-1/2}) e^{- x}}
         = sup \frac {\sqrt{x + 4}} {C (2 + x^{1/2})}$$
Let $y = \frac {\sqrt{x + 4}} {C (2 + x^{1/2})}$ and $y' = 0$.
Solve it and we get $x = 4$ 
Therefore, 
$$\alpha = \frac {\sqrt2} {2C}$$
For the generation of pseudo-random sample from f, we first sample $X \sim g(x)$ and $U \sim U(0, 1)$. Then compare the values of $U$ and $\frac{q(x)}{\alpha g(x)}$: If U is bigger, return to the first step; return X otherwise.\
The returned value is a random sample from the density function $f(x)$.

```{r}
X <- sample_g(5)
theta = 5
C <- 1 / (2 * gamma(theta) + gamma(theta + 0.5))
alpha <- sqrt(2) / (2 * C)
U <- runif(10000, 0, 1)

q <- function(x, theta = 5){
  return(sqrt(4+x)*x^(theta-1)*exp(-x))
}
g_x <-function(x){
  return(C*g(x))
}


count = 1
accept <- c()

while(count <= 10000){
  test_u = U[count]
  test_x = q(X[count])/(alpha*g_x(X[count]))
  if(test_u <= test_x){
    accept = rbind(accept, X[count])
    count = count + 1
  }
  count = count + 1
}

hist(accept,prob=TRUE,main = "Estimated kernel density with theta=5")
lines(density(accept))

curve(q,from = 0,to=20,add=T,col="red")
legend("topright", inset=.1, 
         legend = c("kernel density","true density"), 
         bty = "n", lty = 1, lwd = 2, col = c("black", "red"))

```

## Exercise 6.3.1

### In this exercise we are given the mixture of Normal distributions with unknown parameters of mean and their weights. But we know their priors and the fact that priors are independent. We need to design an MCMC using the Gibbs sampling approach to estimate all 5 parameters.

First, let us generate some data from Normal mixture distribution. Where both means and mixing rate are unknown and to be estimated from data. Note that instead of mixing rate I estimated weights separately and denoted them as $\pi$.
```{r}
set.seed(33)

rmix = function(n,pi,mu,s){
  z = sample(1:length(pi),prob=pi,size=n,replace=TRUE)
  x = rnorm(n,mu[z],s[z])
  return(x)
}
x = rmix(n=1000,pi=c(0.3,0.7),mu=c(-2,2),s=c(1,1))
hist(x)
```

Now we want to infer parameters of mean and weights. That is to say that we want to sample from $p(\mu,\pi|x)$. So the general algorithm looks like this: \ 
1) sample $\mu$ from $\mu|x,z,\pi$; \
2) sample $\pi|x,z,\mu$ \
3) sample $z|x,\pi,\mu$. \

```{r}
normalize = function(x){return(x/sum(x))}
  
  
  sample_z = function(x,pi,mu){
    dmat = outer(mu,x,"-") 
    p.z.given.x = as.vector(pi) * dnorm(dmat,0,1) 
    p.z.given.x = apply(p.z.given.x,2,normalize) 
    z = rep(0, length(x))
    for(i in 1:length(z)){
      z[i] = sample(1:length(pi), size=1,prob=p.z.given.x[,i],replace=TRUE)
    }
    return(z)
  }
 
  sample_pi = function(z,k){
    counts = colSums(outer(z,1:k,FUN="=="))
    pi = gtools::rdirichlet(1,counts+1)
    return(pi)
  }

  sample_mu = function(x, z, k, prior){
    df = data.frame(x=x,z=z)
    mu = rep(0,k)
    for(i in 1:k){
      sample.size = sum(z==i)
      sample.mean = ifelse(sample.size==0,0,mean(x[z==i]))
      
      post.prec = sample.size+prior$prec
      post.mean = (prior$mean * prior$prec + sample.mean * sample.size)/post.prec
      mu[i] = rnorm(1,post.mean,sqrt(1/post.prec))
    }
    return(mu)
  }
  
  gibbs = function(x,k,niter =1000,muprior = list(mean=0,prec=100)){
    pi = rep(1/k,k) # initialize
    mu = rnorm(k,0,10)
    z = sample_z(x,pi,mu)
    res = list(mu=matrix(nrow=niter, ncol=k), pi = matrix(nrow=niter,ncol=k), z = matrix(nrow=niter, ncol=length(x)))
    res$mu[1,]=mu
    res$pi[1,]=pi
    res$z[1,]=z 
    for(i in 2:niter){
        pi = sample_pi(z,k)
        mu = sample_mu(x,z,k,muprior)
        z = sample_z(x,pi,mu)
        res$mu[i,] = mu
        res$pi[i,] = pi
        res$z[i,] = z
    }
    return(res)
  }
```

Now let us depict obtained results as a time series.

```{r}
res = gibbs(x,2)
plot(res$mu[,1],ylim=c(-4,4),type="l")
lines(res$mu[,2],col=2)

```

We can see that MCMCs converge to it's stationary distribution, i.e distributions of $\mu$s. Let us recall that the true values were $-2$ and $2$. Now let us see the results of weights in a mixture.

```{r}
plot(res$pi[,1],ylim = c(-1,1), type="l")
lines(res$pi[,2],col=2)
```

As it can be seen, empirical results are quite close to the real ones of $0.3$ and $0.7$  that were chosen.